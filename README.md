# HeyLisa Backend

Backend Python (FastAPI) de **HeyLisa** â€” Assistante IA exÃ©cutive.  
Ce service constitue le **socle backend applicatif** (logique mÃ©tier, accÃ¨s DB, orchestration), distinct :
- du frontend mobile (Expo / React Native)
- des workflows n8n (automations, webhooks)
- de Supabase (auth + base de donnÃ©es managÃ©e)

---

## ğŸ¯ Objectifs du backend (vision)

- Fournir des **endpoints applicatifs stables** (Context Loader, Quota, Lisa runtime, etc.)
- Centraliser la **logique mÃ©tier critique** (quotas, droits, modes, rÃ¨gles)
- Garantir un accÃ¨s **DB asynchrone performant** (asyncpg)
- ÃŠtre dÃ©ployable facilement (Railway dev/prod)

âš ï¸ Ã€ ce stade, le backend est volontairement **minimal** : on pose le socle proprement avant dâ€™empiler la logique.

---

## ğŸ§± Stack technique

- **Python 3.11**
- **FastAPI** â€” framework API
- **Uvicorn** â€” serveur ASGI
- **asyncpg** â€” driver PostgreSQL asynchrone (choix actÃ©)
- **pydantic-settings** â€” gestion des variables dâ€™environnement
- **structlog** â€” logging structurÃ©
- **Supabase Postgres** â€” base de donnÃ©es (externe)

---

## ğŸ“ Structure du projet

heylisa-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/            # Routes API (ex: health)
â”‚   â”œâ”€â”€ core/           # Config & logging
â”‚   â”œâ”€â”€ init.py
â”‚   â””â”€â”€ main.py         # EntrÃ©e FastAPI
â”‚
â”œâ”€â”€ heylisa-n8n/        # Assets / flows n8n liÃ©s (hors scope backend pur)
â”œâ”€â”€ supabase_schema_prod.sql
â”‚
â”œâ”€â”€ .env                # Variables locales (non versionnÃ©)
â”œâ”€â”€ .env.example        # Template dâ€™env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ runtime.txt         # Version Python pour Railway
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

## âš™ï¸ Setup local

### 1) Environnement Python

```bash
python3 -m venv .venv

#### Activer l'environnement virtuel
source .venv/bin/activate

VÃ©rification :

which python
python3 --version
# => Python 3.11.x depuis .venv

2) Installation des dÃ©pendances
pip install -r requirements.txt

Contenu actuel :
fastapi
uvicorn[standard]
python-dotenv
pydantic-settings
structlog
httpx
asyncpg

3) Variables dâ€™environnement

CrÃ©er .env Ã  la racine (ne jamais committer) :

DATABASE_URL=postgresql://postgres:PASSWORD@db.<project-ref>.supabase.co:5432/postgres
ENVIRONMENT=dev
LOG_LEVEL=INFO

âš ï¸ Important :
	â€¢	Ne pas mettre de crochets [] autour du password dans lâ€™URL (sinon asyncpg casse).
	â€¢	Pour Supabase : choisir Direct connection pour usage â€œservice backend / long-livedâ€.
	â€¢	En dev local, lâ€™IP allowlist Supabase peut Ãªtre requise selon ta config.

ğŸ‘‰ DATABASE_URL correspond Ã  la connection string Supabase
(Supabase â†’ Settings â†’ Database â†’ Connection string).

â¸»

4) Lancer le serveur en local (commande standard)

âš ï¸ Commande officielle recommandÃ©e (Ã©vite les soucis de PATH) :

python3 -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

âš ï¸ Commande simple
uvicorn app.main:app --reload --port 8000

âœ… Health check

Endpoints :
GET /health

Test : 
curl -s http://127.0.0.1:8000/health | python3 -m json.tool


RÃ©ponse attendue : 
{
  "status": "healthy",
  "environment": "dev",
  "version": "0.1.0",
  "timestamp": "2026-02-06T02:45:14.538215"
}

GET /v1/quota/{public_user_id}

Test : 
curl -s http://127.0.0.1:8000/v1/quota/<PUBLIC_USER_ID> | python3 -m json.tool

Retourne lâ€™Ã©tat quota dâ€™un user (read-only) :

RÃ©ponse :
{
  "public_user_id": "...",
  "is_pro": false,
  "free_quota_used": 6,
  "free_quota_limit": 8,
  "state": "normal",
  "paywall_should_show": false
}

VOIR LES LOGS DU CHAT

FULL LOG : 
En local :
LOG_LEVEL=DEBUG python3 -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

FiltrÃ© : 
python3 -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 --access-log --log-level debug | grep -E "chat_intro|chat_message|heylisa_backend"

python3 -m uvicorn app.main:app --reload --port 8000 --log-level info


Et si tu veux les prompts en fichiers (Step 2 quand on lâ€™ajoute cÃ´tÃ© ResponseWriter) :
LOG_LEVEL=DEBUG DEBUG_PIPELINE=1 DEBUG_DUMP_PROMPTS=1 python3 -m uvicorn ...

A) Voir tout le chat tracing
python3 -m uvicorn app.main:app --reload --port 8000 | grep heylisa.chat

B) Voir uniquement les events (encore plus strict)
python3 -m uvicorn app.main:app --reload --port 8000 | grep '"logger": "heylisa.chat"'

C) Voir juste les nodes
python3 -m uvicorn app.main:app --reload --port 8000 | grep '"event": "chat.node.'

(sur mac, grep marche direct)


RÃ¨gles Quota (v1)

Tables utilisÃ©es
	â€¢	public.users.is_pro (source de vÃ©ritÃ© abonnement)
	â€¢	public.user_settings.free_quota_used (compteur)
	â€¢	public.user_settings.free_quota_limit (limit)

Invariants
	â€¢	On ne reset jamais free_quota_used (quota free â€œlifetimeâ€)
	â€¢	state calculÃ© backend (aide Lisa + logique cÃ´tÃ© services) :
	â€¢	normal si used < limit - 1
	â€¢	warn_last_free si used == limit - 1 (ex: message #7 si limit=8)
	â€¢	blocked si used >= limit

Paywall
	â€¢	Le front doit afficher paywall si :
	â€¢	!isPro && free_quota_used >= free_quota_limit
	â€¢	Câ€™est volontairement un pont direct DB <-> front en realtime (option A).
	â€¢	Le backend sert surtout Ã  fournir un Ã©tat consolidÃ© (state) pour Lisa / services.

ğŸ“œ Journal dâ€™implÃ©mentation

2026-02-06 â€” Backend v0 stabilisÃ©

But
	â€¢	Poser un socle backend propre avant toute logique mÃ©tier.
	â€¢	PrÃ©parer lâ€™intÃ©gration future des endpoints (Quota, Context Loader, Lisa).

RÃ©alisÃ©
	â€¢	Initialisation FastAPI fonctionnelle
	â€¢	Logging structurÃ© en place
	â€¢	Endpoint /health opÃ©rationnel
	â€¢	Environnement Python isolÃ© (.venv)
	â€¢	Choix technique actÃ© : asyncpg pour PostgreSQL
	â€¢	Commandes de run standardisÃ©es (python3 -m uvicorn)
	â€¢	CompatibilitÃ© Railway (runtime.txt)

DÃ©cisions techniques clÃ©s
	â€¢	DB driver : asyncpg (asynchrone, performant)
	â€¢	Backend volontairement minimal au dÃ©part
	â€¢	Documentation tenue au fil de lâ€™eau (pas de dette doc)

â¸»

ğŸš§ Ce qui nâ€™est PAS encore implÃ©mentÃ© (volontairement)
	â€¢	Pool de connexion DB
	â€¢	Endpoints mÃ©tier (quota, context loader, etc.)
	â€¢	Auth backend (repose encore sur Supabase cÃ´tÃ© front)
	â€¢	SÃ©curitÃ© avancÃ©e (RLS backend, scopes, etc.)

ğŸ‘‰ Ces Ã©lÃ©ments seront ajoutÃ©s Ã©tape par Ã©tape, chacun documentÃ© dans ce journal.

â¸»

â–¶ï¸ Prochaines Ã©tapes prÃ©vues
	1.	Ajout du module DB asyncpg (pool)
	2.	Service Quota standalone (sans branchement front)
	3.	Endpoint /quota/status
	4.	Puis intÃ©gration progressive au Context Loader

â¸»

ğŸ§  RÃ¨gle de gouvernance

Toute Ã©volution backend doit :
	â€¢	Ãªtre commitÃ©e
	â€¢	Ãªtre documentÃ©e ici (quoi / pourquoi / contraintes)
	â€¢	ne pas casser lâ€™existant sans dÃ©cision explicite


## ğŸ§  Chat Engine â€” Ã‰tat actuel (fÃ©vrier 2026)

Architecture gÃ©nÃ©rale
	â€¢	Frontend (Expo / React Native)
	â€¢	UI chat optimiste + DB source of truth
	â€¢	Gestion fine des Ã©tats :
	â€¢	isLisaBusy
	â€¢	isLisaThinking
	â€¢	isSlowThinking
	â€¢	Aucun message Lisa nâ€™est Ã©crit cÃ´tÃ© front (sauf fallback local UX)
	â€¢	Backend (API Chat)
	â€¢	Endpoint principal :
	POST /v1/chat/message


	â€¢	Le backend est lâ€™unique source de vÃ©ritÃ© pour :
	â€¢	la crÃ©ation des messages Lisa
	â€¢	la persistance en base
	â€¢	la logique mÃ©tier (quota, routing, agents Ã  venir)

â¸»

Flux dâ€™un message utilisateur
	1.	Lâ€™utilisateur envoie un message depuis le front
	2.	Le message est sauvegardÃ© immÃ©diatement en base (conversation_messages)
	3.	Le front affiche le message en optimiste
	4.	Le front appelle :
POST /v1/chat/message

avec :

{
  "conversation_id": "...",
  "user_message_id": "..."
}

	5.	Le backend traite le message (logique en cours dâ€™extension)
	6.	Le front recharge lâ€™historique depuis la DB
ğŸ‘‰ UI 100% alignÃ©e DB, zÃ©ro divergence

â¸»

Gestion des erreurs (front)
	â€¢	Erreurs rÃ©seau / front
	â€¢	Pas de fallback backend
	â€¢	Pas dâ€™Ã©criture DB
	â€¢	Message UX local Lisa :
â€œJe nâ€™arrive pas Ã  joindre le serveur. RÃ©essaie dans quelques secondes ğŸ™â€
	â€¢	Le message utilisateur est restaurÃ© dans le champ si non sauvegardÃ©
	â€¢	Erreurs backend
	â€¢	Fallback Lisa local (pas DB)
	â€¢	Aucun Ã©tat bloquant (watchdog UI)
	â€¢	Watchdogs
	â€¢	Soft warning aprÃ¨s 25s (isSlowThinking)
	â€¢	Hard UI release aprÃ¨s 5 min (anti â€œLisa thinking infiniâ€)

â¸»

Configuration environnement
	â€¢	Le frontend utilise dynamiquement :

BACKEND_BASE_URL

injectÃ© via :
	â€¢	app.config.ts
	â€¢	extra.backend.baseUrl
	â€¢	fallback local http://127.0.0.1:8000

	â€¢	En production :
https://api.heylisa.io

ğŸ‘‰ Aucun changement front requis entre dev / prod.

â¸»

Ã‰tat de stabilitÃ©
	â€¢	âœ… Chat fonctionnel en DEV
	â€¢	âœ… Paywall backend-compatible
	â€¢	âœ… UX fluide (typing, scroll, erreurs)
	â€¢	âœ… Architecture validÃ©e pour extension (agents, routing, onboarding)


## Pilotage des LLMs : rÃ¨gle actÃ©e (simple et saine)

1ï¸âƒ£ Choix des providers

On grave Ã§a dans le marbre :

Ordre dâ€™appel
	1.	DeepSeek â†’ provider primaire
	2.	OpenAI 4o-mini â†’ fallback uniquement

Principe
	â€¢	Le backend ne sait pas â€œquel agentâ€ utilise quel LLM.
	â€¢	Il appelle un LLM runtime unique.
	â€¢	Ce runtime :
	â€¢	tente DeepSeek
	â€¢	si erreur / timeout / output invalide â†’ fallback OpenAI
	â€¢	renvoie { text, provider }

ğŸ‘‰ Tu lâ€™as dÃ©jÃ  implicitement fait dans le chat engine, on ne change rien, on gÃ©nÃ©ralise.

â¸»

2ï¸âƒ£ TrÃ¨s bon point : ne PAS figer les outputs de tous les agents

Tu as 100% raison.

âŒ Erreur classique Ã  Ã©viter

â€œOn dÃ©finit dÃ¨s maintenant les JSON outputs de 12 agents quâ€™on nâ€™a pas encore vraiment Ã©prouvÃ©sâ€

RÃ©sultat habituel :
	â€¢	rigiditÃ© prÃ©maturÃ©e
	â€¢	refactors incessants
	â€¢	perte de vitesse

â¸»

3ï¸âƒ£ La bonne stratÃ©gie (ce que je te recommande fortement)

âœ… Ce quâ€™on fixe MAINTENANT

Seulement les invariants systÃ©miques, pas les mÃ©tiers :

A. Convention universelle dâ€™output agent

Tous les agents doivent respecter au moins ceci :

{
  "confidence": 0.0,
  "decision": "...",
  "notes": "...",
  "payload": {}
}

	â€¢	confidence âˆˆ [0,1] â†’ obligatoire
	â€¢	decision â†’ string courte (routing, choix contexte, etc.)
	â€¢	notes â†’ explicatif humain (debug / logs / observabilitÃ©)
	â€¢	payload â†’ libre, Ã©volutif, spÃ©cifique Ã  lâ€™agent

ğŸ‘‰ Le backend ne fait confiance Ã  un agent que si confidence â‰¥ 0.8
Sinon â†’ fallback dÃ©terministe (LIGHT context, rÃ©ponse safe, etc.)

Câ€™est LA rÃ¨gle critique


â¸»

B. System messages & prompts : figÃ©s, mais extensibles

On fige :
	â€¢	la structure
	â€¢	la philosophie
	â€¢	les garde-fous

On nâ€™Ã©numÃ¨re pas :
	â€¢	tous les agents
	â€¢	tous les champs
	â€¢	tous les cas

â¸»

4ï¸âƒ£ Organisation concrÃ¨te des prompts (v1 rÃ©aliste)

app/prompts/
  system/
    lisa_persona.md
    safety.md
    output_contract.md   # â† le JSON minimal ci-dessus
  agents/
    intent_classifier.md
    router.md
    onboarding.md
    response_generator.md


	â€¢	Chaque prompt est indÃ©pendant
	â€¢	Le backend compose dynamiquement
	â€¢	Aucun prompt â€œgÃ©ant universelâ€

â¸»

5ï¸âƒ£ Comment le backend orchestre sans sur-spÃ©cifier


 âœ… Backend agentique (Orchestration)

Request
    â†“
Orchestrator (dÃ©cide du plan)
    â†“
Agent Graph (DAG - Directed Acyclic Graph)
    â”œâ”€â†’ Agent A (parallÃ¨le)
    â”œâ”€â†’ Agent B (parallÃ¨le)
    â”‚     â†“
    â””â”€â†’ Agent C (sÃ©quentiel, dÃ©pend de A+B)
          â†“
       Agent D (synthÃ¨se)
    â†“
Response

Non-linÃ©aire, adaptatif, asynchrone


Mico-Exemple logique simplifiÃ©e (pseudo-flow)

message user
   â†“
IntentClassifierAgent
   â†’ confidence < 0.8 ? fallback general
   â†“
RouterAgent
   â†’ choisit context_level (light / medium / max)
   â†’ confidence < 0.8 ? force light
   â†“
ContextLoader(context_level)
   â†“
ResponseGeneratorAgent


Ã€ aucun moment :
	â€¢	le backend nâ€™impose une structure mÃ©tier rigide
	â€¢	le backend ne â€œdevineâ€ Ã  la place de Lisa

ğŸ‘‰ Lisa dÃ©cide, le backend valide et borne.

â¸»

6ï¸âƒ£ OÃ¹ vivent les rÃ¨gles mÃ©tier (important)

Ã‰lÃ©ment
OÃ¹
Persona Lisa > system prompt
Anti-patterns > system prompt
Choix contexte > agent router
Limites quota > backend
Fallback sÃ©curitÃ© > backend
Langue / timezone > context loader
ProactivitÃ© > agent + cron / jobs

ğŸ‘‰ Le backend est le gendarme, pas lâ€™intelligence.

â¸»

7ï¸âƒ£ Ce qui est verrouillÃ© 

On a :
	â€¢	un runtime LLM propre
	â€¢	une architecture agentique contrÃ´lÃ©e
	â€¢	une Ã©volution incrÃ©mentale possible
	â€¢	zÃ©ro dette de prompt prÃ©maturÃ©e
	â€¢	une Lisa qui reste maÃ®tresse de ses dÃ©cisions


---

## ğŸ“œ Journal dâ€™implÃ©mentation

### 2026-02-07 â€” Chat Engine v1 (Orchestrator + PlanExecutor + ResponseWriter) âœ…

Objectif
- Passer dâ€™un â€œchat direct LLMâ€ Ã  une architecture **agentique contrÃ´lÃ©e** (DAG) :
  - Orchestrator = dÃ©cide intent / contexte / besoin web / plan
  - PlanExecutor = exÃ©cute le plan (tools + agents)
  - ResponseWriter (Lisa) = gÃ©nÃ¨re la rÃ©ponse finale avec conventions UI stables

RÃ©alisÃ©
- âœ… **OrchestratorAgent** (LLM) qui produit un plan DAG JSON :
  - DÃ©tection `intent`, `context_level`, `need_web`, `web_search_prompt`
  - GÃ©nÃ©ration du plan via une **whitelist stricte** de nodes
  - Guardrails : confidence, cohÃ©rence need_web, contraintes amabilities
- âœ… **PlanExecutor** :
  - ExÃ©cution topo simple basÃ©e sur `depends_on`
  - ExÃ©cution des nodes autorisÃ©s :
    - `tool.db_load_context` (context loader)
    - `tool.quota_check` (statut quota)
    - `tool.web_search` (Perplexity sonar-pro)
    - `agent.response_writer` (Lisa)
  - **Verrou â€œanswer-onlyâ€** : sortie toujours une string safe (fallback + truncation)
  - **Hard allowlist** cÃ´tÃ© executor : tout node non autorisÃ© est rejetÃ©
- âœ… **ResponseWriterAgent (Lisa)** :
  - System prompt stable + anti-patterns + rÃ¨gles intent
  - **Conventions UI** pour rÃ©ponses â€œchat-safeâ€ (pas de HTML, pas de markdown complexe)
  - Gestion des sources web :
    - Affiche au besoin un bloc `ğŸ“Œ Sources` (1 Ã  3 puces, sans URL)
    - Nâ€™injecte au modÃ¨le que titres + domaines (pas de liens bruts)
  - Compat params : `web=` + fallback `web_search=`

DÃ©cisions techniques clÃ©s
- **Architecture agentique (DAG) contrÃ´lÃ©e**
  - Le backend exÃ©cute, borne et valide.
  - Lâ€™intelligence est rÃ©partie : orchestrator (plan) + lisa (rÃ©daction).
- **Whitelisting strict des nodes**
  - Source de vÃ©ritÃ© : `app/agents/node_registry.py`
  - UtilisÃ© par :
    - lâ€™Orchestrator (dans le prompt + validation)
    - le PlanExecutor (refus hard si type non autorisÃ©)
- **IDs A/B/C/D**
  - Convention simple pour v0 :
    - A = context
    - B = quota
    - C = web_search (si besoin)
    - D = response_writer (final)
  - Lâ€™ordre de traitement rÃ©el reste dÃ©terminÃ© par `depends_on` (pas par la lettre).

Endpoints impactÃ©s
- `POST /v1/chat/message`
  - Devient le point dâ€™entrÃ©e unique :
    - lit message user (DB)
    - orchestre plan
    - exÃ©cute
    - Ã©crit le message Lisa en DB (source of truth)

Gestion des erreurs
- **Niveau Chat (chat.py)**
  - Try/except global : fallback rÃ©ponse safe si crash complet
- **Niveau PlanExecutor**
  - Verrou final â€œanswer-onlyâ€ : mÃªme si response_writer foire, on renvoie une string safe
  - Anti pavÃ© : limite `MAX_ANSWER_CHARS` + ellipsis

Fichiers ajoutÃ©s / modifiÃ©s (v1)
- `app/services/chat.py`
  - Branche OrchestratorAgent + PlanExecutor
  - Persistance DB + idempotence via `assistant_message_id` + `dedupe_key`
- `app/agents/orchestrator.py`
  - Prompt de routing + gÃ©nÃ©ration plan DAG
  - Validation plan + fallback minimal
  - IntÃ©gration whitelist via `node_registry`
- `app/services/plan_executor.py`
  - ExÃ©cution DAG + guardrails answer-only
  - Hard allowlist `NODE_TYPE_WHITELIST`
- `app/agents/response_writer.py`
  - Lisa â€œwriterâ€ : conventions UI + rÃ¨gles intent + sources digest
- `app/tools/web_search.py`
  - Tool web search (Perplexity sonar-pro) JSON strict
- `app/agents/node_registry.py`
  - Source de vÃ©ritÃ© : whitelist node types + rÃ¨gles IDs
  - Helpers de rendu pour inclusion dans le system prompt

Ce qui nâ€™est PAS encore implÃ©mentÃ© (volontairement)
- âŒ ExÃ©cution dâ€™actions rÃ©elles (Ultimate mode : agenda, email, etc.)
- âŒ â€œPro modesâ€ (Medical/Airbnb/etc.) branchÃ©s au routing
- âŒ ParallÃ©lisation rÃ©elle (parallel_group ignorÃ© en v0)
- âŒ ObservabilitÃ© avancÃ©e (traces structurÃ©es par node, metrics, etc.)

Prochaines Ã©tapes prÃ©vues
1. Verrouiller les prompts de lâ€™Orchestrator (tests non-rÃ©gression par intent)
2. Optimisation ResponseWriter (Lisa) :
   - cadrage des entrÃ©es (context, web, intent)
   - anti-robot + concision + style stable
3. (Option) Ajout dâ€™un â€œdebug modeâ€ backend (stockage exec_out.debug en metadata)
4. (Option) ParallÃ©lisation vraie des nodes `parallel_group`

---

## Scope of work - Flow Orchestrateur

1) Ce que fait VRAIMENT lâ€™orchestrateur

Il fait 3 choses, dans cet ordre :

A) Lecture dâ€™Ã©tat (state)

Ã€ partir du contexte (profil, quota, facts, abonnements, historique), il dÃ©duit :
	â€¢	eligibility.smalltalk_intro = true/false
	â€¢	capabilities (ce que Lisa a le droit de faire selon les abonnements)
	â€¢	conversation_phase (ex: intro onboarding / conversation normale)
	â€¢	topic continuity (le user dit â€œokâ€ => on garde lâ€™intent prÃ©cÃ©dent ou on suit une continuitÃ©)

B) DÃ©cision dâ€™intent (intent + mode)

Il choisit un intent en tenant compte de la dynamique (last 10 messages), pas juste le dernier.

C) Application de guardrails business (gates)

Il applique les rÃ¨gles â€œnon nÃ©gociablesâ€ :
	â€¢	smalltalk_intro prioritaire si Ã©ligible ET pas dÃ©viÃ© par intent prioritaire
	â€¢	certains intents interdits si pas dâ€™abonnement => on garde lâ€™intent â€œnaturelâ€, mais on le marque non Ã©ligible et on bascule en â€œdowngrade behaviorâ€ (rÃ©ponse adaptÃ©e + upsell soft si besoin)
	â€¢	quand smalltalk_intro est actif, small_talk et amabilities sont dÃ©sactivÃ©s (ou plutÃ´t absorbÃ©s par le mode intro)

â¸»

2) Les entrÃ©es indispensables (ctx) â€” version â€œsource of truthâ€

I- Contexte minimum :

user_profile
	â€¢	public_user_id
	â€¢	first_name, last_name
	â€¢	locale_main, timezone
	â€¢	use_tu_form (bool|null)

user_status
	â€¢	is_pro (OK mais pas suffisant)
	â€¢	free_quota_used (count messages user lifetime)
	â€¢	free_quota_limit (8)
	â€¢	state: normal | warn_last_free | blocked

user_facts_required
	â€¢	required_keys = [first_name, use_tu_form, main_city, main_activity]
	â€¢	known / missing + missing_count

subscriptions / capabilities / Integrations

(Ã€ partir de public.lisa_user_agents (ton screen) :
	â€¢	agents actifs (ex: personal_assistant)
	â€¢	donc capabilities calculÃ©es :
	â€¢	can_action_request
	â€¢	can_deep_work
	â€¢	can_professional_modes
	â€¢	etc.)

history
	â€¢	last_10 messages (role + content + ts + sender)
	â€¢	last_user_message (string)
	â€¢	last_assistant_message (string)
	â€¢	optional: last_orchestrator_intent (si tu le stockes en metadata)

II- Contexte additionnel

Principes (simples, robustes)

A. Contexte = proportionnel Ã  lâ€™utile
	â€¢	Si le user dit â€œmerci bonne nuitâ€, on ne charge pas le bilan comptable 2024 et lâ€™astrologie.
	â€¢	Si le user dÃ©marre (smalltalk_intro), on charge minimum vital pour collecter les facts.

B. Le contexte est une dÃ©cision orchestrateur, mais le backend garde le sifflet
	â€¢	Orchestrateur propose context_level
	â€¢	Backend force certains cas (ex : user non pro â†’ pas de medium/max)

RÃ¨gles v1 (ancrÃ©es)
	â€¢	Si intent = smalltalk_intro â†’ context_level = light (forcÃ©)
	â€¢	Si is_pro = false â†’ context_level âˆˆ {light} (forcÃ©)
	â€¢	Si is_pro = true et user a agent perso (personal/ultimate)
â†’ medium possible, selon intent, sinon light.
	â€¢	Si user a un agent â€œpro modeâ€ actif (medical_assistant, etc.)
â†’ max possible, selon intent + question, sinon medium/light.

Extension future (dÃ©jÃ  prÃ©vue)
	â€¢	Orchestrateur peut activer un node SQL ciblÃ© (â€œfetch something preciseâ€), injectÃ© dans le contexte (mais on garde Ã§a pour aprÃ¨s, pas dans la clÃ´ture smalltalk).

	ğŸ§  Context Management â€” v1 (HeyLisa)

Objectif

Garantir que Lisa reÃ§oive le bon niveau de contexte, ni plus ni moins, en fonction :
	â€¢	de lâ€™intention utilisateur,
	â€¢	de son stade (nouvel utilisateur vs habituÃ©),
	â€¢	de ses capacitÃ©s / abonnements.

Principe fondamental :

Le contexte est proportionnel Ã  lâ€™utile.

â¸»

1. Architecture gÃ©nÃ©rale des contextes

Le contexte est structurÃ© en 2 blocs distincts :

I. Bloc minimum (toujours chargÃ©)

Ce bloc est la source of truth.
Il est chargÃ© dans tous les cas, quel que soit lâ€™intent ou le niveau.

user_profile
	â€¢	public_user_id
	â€¢	first_name
	â€¢	last_name
	â€¢	full_name

settings
	â€¢	locale_main
	â€¢	timezone
	â€¢	use_tu_form (bool | null)
	â€¢	intro_smalltalk_turns
	â€¢	intro_smalltalk_done
	â€¢	main_city
	â€¢	main_activity

user_status
	â€¢	is_pro
	â€¢	free_quota_used
	â€¢	free_quota_limit
	â€¢	state : normal | warn_last_free | blocked

history
	â€¢	messages : 10 derniers messages (ordre chronologique)
	â€¢	role
	â€¢	content
	â€¢	sent_at

user_facts (logique smalltalk)
	â€¢	required_keys
	â€¢	known
	â€¢	missing_required
	â€¢	missing_required_count

gates
	â€¢	smalltalk_intro_eligible
	â€¢	smalltalk_target_key
	â€¢	missing_required

â¡ï¸ Ce bloc est stable, minimal, et ne doit jamais Ãªtre cassÃ©.

â¸»

2. Bloc additionnel â€” niveaux de contexte

Le bloc additionnel est variable, dÃ©cidÃ© par lâ€™orchestrateur mais contrÃ´lÃ© par le backend.

Niveaux disponibles

Niveau Description
light -> Contexte minimal utile
medium -> Contexte enrichi (assistant personnel)
max -> Contexte professionnel profond (modes pro)

4. DÃ©finition officielle du Context Light (v1)

Le contexte light inclut :

4.1 Bloc minimum (cf. section 1)

Toujours inclus.

4.2 Facts persistÃ©s (DB)

ChargÃ©s depuis public.user_facts.

Structure exposÃ©e au modÃ¨le :

facts_store: {
  count: number,
  items: [
    {
      fact_key: string,
      category: string,
      scope: string,
      value_type: string,
      value: any,
      confidence: number,
      is_estimated: boolean,
      source_ref: string | null,
      notes: string | null,
      updated_at: string
    }
  ],
  keys: string[]
}

ğŸ“Œ Important :
	â€¢	Les valeurs rÃ©elles sont bien prÃ©sentes (main_city = "Paris", etc.)
	â€¢	facts_keys_sample sert au debug, pas Ã  lâ€™agent.
	â€¢	Lâ€™agent raisonne sur items.value, pas sur les clÃ©s seules.

â¸»

5. RÃ´le du Context Light en SmalltalkIntro

Le contexte light est le contexte de rÃ©fÃ©rence du SmalltalkIntro.

Il permet :
	â€¢	de connaÃ®tre ce qui est dÃ©jÃ  su sur lâ€™utilisateur,
	â€¢	dâ€™identifier le prochain fact prioritaire Ã  collecter,
	â€¢	de guider une conversation naturelle sans surcharger le modÃ¨le.

ğŸ‘‰ MÃªme un utilisateur ancien peut repasser en light context
si lâ€™intent est trivial ("merci", "bonne nuit").

â¸»

6. Ce que le Context Light ne fait pas
	â€¢	âŒ Ne charge pas de donnÃ©es mÃ©tier lourdes
	â€¢	âŒ Ne charge pas dâ€™historique long
	â€¢	âŒ Ne dÃ©clenche pas de SQL ciblÃ©
	â€¢	âŒ Ne suppose aucun besoin professionnel

Ces extensions sont prÃ©vues dans medium / max, ultÃ©rieurement.

â¸»

7. Ã‰tat de validation
	â€¢	âœ… Context light implÃ©mentÃ©
	â€¢	âœ… Facts DB chargÃ©s avec valeurs
	â€¢	âœ… Logs explicites (facts_store_count, facts_keys_sample)
	â€¢	âœ… SmalltalkIntro fonctionnel et traÃ§able
	â€¢	ğŸ”’ Contrat figÃ© pour v1
	
â¸»

3) RÃ¨gle clÃ© : smalltalk_intro = un MODE, pas juste un intent

On introduit 2 notions :

(1) eligibility.smalltalk_intro

DÃ©terministe :
	â€¢	free_quota_used < 8 ET
	â€¢	missing_required_count > 0

Note : tu as raison, ne pas conditionner Ã  is_pro, car on peut sâ€™abonner avant dâ€™atteindre 8. Donc le quota gating prime.

(2) mode.lock = smalltalk_intro (soft lock)

Si eligible, alors le mode par dÃ©faut devient smalltalk_introâ€¦
â€¦sauf si la conversation dÃ©vie vers un intent prioritaire.

Donc on a une rÃ¨gle :

Smalltalk Intro doit sâ€™appliquer si :
	â€¢	eligible.smalltalk_intro = true
	â€¢	ET pas de signal â€œoverrideâ€ (voir section 4)

â¸»

4) Overrides : quand on casse le smalltalk_intro mÃªme si Ã©ligible

On dÃ©finit une liste dâ€™intents qui cassent le mode intro (au moins pour ce tour) :

PrioritÃ© haute (toujours override) :
	â€¢	urgent_request
	â€¢	sensitive_question

PrioritÃ© moyenne (override si explicite) :
	â€¢	functional_question (ex: â€œtu fais quoi ?â€, â€œcomment tu fonctionnes ?â€)
	â€¢	professional_request (si user parle patient, rÃ©servation, etc.)
(mais lÃ  tu vas souvent Ãªtre non Ã©ligible cÃ´tÃ© capabilities)

PrioritÃ© â€œflow naturelâ€ (override si lâ€™utilisateur part vraiment en tÃ¢che) :
	â€¢	decision_support
	â€¢	motivational_guidance
	â€¢	general_question

Et cas spÃ©cial : le user dit â€œokâ€, â€œouiâ€, â€œnonâ€, â€œvas-yâ€, â€œnickelâ€.
LÃ  lâ€™orchestrateur doit regarder le tour prÃ©cÃ©dent :

	â€¢	si on Ã©tait en smalltalk_intro => on continue smalltalk_intro
	â€¢	sinon on continue lâ€™intent prÃ©cÃ©dent / thread actif

â¸»

5) DÃ©sactivation de certains intents en mode intro

Tu lâ€™as dit : quand eligible.smalltalk_intro = true, on dÃ©sactive :
	â€¢	amabilities (un â€œmerciâ€ pendant lâ€™intro, on le traite comme une micro-politesse MAIS on reste en intro)
	â€¢	small_talk (absorbÃ© par smalltalk_intro)

Donc :
	â€¢	Lâ€™orchestrateur peut toujours dÃ©tecter â€œamabilitiesâ€ comme signal,
	â€¢	mais il ne doit pas retourner intent=amabilities si intro est active,
	â€¢	il retourne intent=smalltalk_intro avec un flag signals.amabilities=true (utile pour writer).

â¸»

6) Capabilities gating (abonnements) â€” ton cas action_request / deep_work / professional_request

Tu as un point trÃ¨s important : le user peut demander une action mÃªme si pas Ã©ligible.

Donc on sÃ©pare :
	â€¢	intent = ce que le user veut
	â€¢	eligible = est-ce quâ€™on a le droit / le mode actif

Exemple

User: â€œRÃ©serve-moi un restoâ€
	â€¢	intent = action_request
	â€¢	eligible = false (si pas agent / abonnement)
	â€¢	behavior = â€œrefuse + propose upgrade / propose alternative (guidage)â€

Donc le rÃ©sultat orchestrateur doit porter :
	â€¢	intent
	â€¢	intent_eligible: bool
	â€¢	intent_block_reason: str|null
	â€¢	mode (smalltalk_intro vs normal)

âš ï¸âš ï¸âš ï¸ IL FAUT RAJOUTER AUSSI LES SERVICES ACTIFS SELON CAPABILITIES (CAR USER PEUT VOIR UN ABONNEMENT MAIS PAS AVOIR CONNECTÃ‰ UN OUTIL NÃ‰CESSAIRE)
â¸»

7) Sortie OrchestratorResult â€” Ã  enrichir

Je te conseille dâ€™Ã©tendre ton JSON (sans exploser le systÃ¨me) :
	â€¢	intent: â€¦
	â€¢	mode: "smalltalk_intro" | "normal"
	â€¢	intent_eligible: true/false
	â€¢	gates: { smalltalk_intro_eligible, smalltalk_intro_locked, blocked_reason }
	â€¢	capabilities: { can_action_request, can_deep_work, can_professional_request }
	â€¢	signals: { short_reply, amabilities, etc. } (optionnel)

Et le plan peut rester minimal, mais avec inputs ResponseWriter enrichis :
	â€¢	inputs.mode
	â€¢	inputs.intent_eligible
	â€¢	inputs.block_reason
	â€¢	inputs.smalltalk_target_key (si intro: quelle fact on collecte maintenant)

â¸»

8) ConcrÃ¨tement : ton â€œStep 1â€ change

Au lieu de â€œOrchestrator = LLM qui classifie et fait un planâ€, on fait :

Partie dÃ©terministe (Python, avant LLM)
	â€¢	compute eligibility.smalltalk_intro
	â€¢	compute capabilities depuis lisa_user_agents
	â€¢	compute conversation signals (short reply, last intent continuity si dispo)

Partie LLM (mais contrainte)
	â€¢	choisir intent en regardant last10 + last_user
	â€¢	MAIS avec rÃ¨gle : si eligible.smalltalk_intro => propose smalltalk_intro sauf override

Puis re-guardrails (Python, aprÃ¨s LLM)
	â€¢	appliquer les gates
	â€¢	forcer intent si nÃ©cessaire
	â€¢	calculer intent_eligible
	â€¢	injecter flags dans plan

âš ï¸âš ï¸âš ï¸ ON GARDE SEULEMENT APPLIQUER LES GATES ET INJECTER LES FLAGS DANS "RE-GUARDRAILS"
â¸»

9) Next step (sans discuter 3h) : ce quâ€™on code maintenant

On avance dans lâ€™ordre que tu voulais :

Ã‰tape 1.1 â€” on ajoute 2 champs dans IntentType
	â€¢	smalltalk_intro
	â€¢	(Ã©ventuellement) blocked (pas obligatoire, mais utile)

Ã‰tape 1.2 â€” on modifie OrchestratorAgent.run pour accepter ctx

Et on ajoute au SYSTEM_PROMPT :
	â€¢	dÃ©finition de mode
	â€¢	rÃ¨gles smalltalk_intro + overrides
	â€¢	rÃ¨gle â€œshort reply => dÃ©pend du contexte prÃ©cÃ©dentâ€

Ã‰tape 1.3 â€” on sort du LLM la dÃ©cision â€œintentâ€ seulement (plan minimal figÃ©)

Franchement : garde ton plan minimal constant pour lâ€™instant.
Le vrai pilotage se fait via inputs de ResponseWriter.

â¸»

ğŸ” Discovery Sequence (v1) â€” mode forcÃ©, guidage, docs scopes

Objectif

La Discovery est une sÃ©quence structurÃ©e qui â€œprend la mainâ€ sur le chat pour :
	â€¢	cadrer lâ€™utilisateur (ce quâ€™il veut, son contexte, ses contraintes),
	â€¢	Ã©tablir un socle de facts utiles,
	â€¢	guider vers une aide efficace sans tourner en rond,
	â€¢	prÃ©parer lâ€™activation dâ€™un mode payant si besoin (Ultimate / Pro modes), sans forcer.

Principes clÃ©s
	â€¢	Discovery est un MODE, pas juste un intent.
	â€¢	Source de vÃ©ritÃ© = ctx.gates (calculÃ© par context_loader, jamais inventÃ© par le LLM).
	â€¢	Si ctx.gates.discovery_forced=true et ctx.gates.discovery_status != "complete", alors Discovery override tout,
sauf urgent_request et sensitive_question.

Contrat cÃ´tÃ© Orchestrator
	1.	LLM propose un intent (et need_web / scopes docs Ã©ventuels).
	2.	Le backend applique des guardrails dÃ©terministes :

	â€¢	mode = "discovery" + intent_final = "discovery" si discovery forced (sauf urgence/sensible)
	â€¢	Absorption des intents â€œsociauxâ€ pendant discovery :
	â€¢	amabilities â†’ reste en discovery
	â€¢	small_talk â†’ reste en discovery

	3.	Le plan nâ€™est pas â€œinventÃ©â€ par le LLM : on construit un plan stable (min-risk) et on injecte les flags.

Documentation interne (docs scopes) pendant Discovery

Discovery peut sâ€™appuyer sur la doc produit, mais de maniÃ¨re contrÃ´lÃ©e :
	â€¢	Les scopes disponibles sont listÃ©s dans le system prompt via ctx.docs.scopes_all.
	â€¢	Lâ€™orchestrator peut activer scope_need=true et choisir scopes_selected (1 Ã  3 max).
	â€¢	Le PlanExecutor exÃ©cute alors tool.docs_chunks (node S) :
	â€¢	Source de vÃ©ritÃ© : ctx.docs.chunks_by_scope[scope]
	â€¢	Hard caps : max 3 scopes, max 20 chunks, max 8 chunks par scope
	â€¢	Le ResponseWriter reÃ§oit docs_chunks et doit les utiliser en prioritÃ© si prÃ©sents (avant le contexte compact).

DAG v1 (plan minimal)

En Discovery, le plan reste stable. Exemple typique :
	â€¢	A: tool.db_load_context
	â€¢	B: tool.quota_check
	â€¢	(S): tool.docs_chunks (si scope_need=true)
	â€¢	(C): tool.web_search (si need_web=true)
	â€¢	D: agent.response_writer (rÃ©ponse finale)

Node IDs convention : A, B, C, S, D
Source de vÃ©ritÃ© whitelist : app/agents/node_registry.py

Inputs injectÃ©s au ResponseWriter (Discovery)

Le ResponseWriter reÃ§oit en entrÃ©e :
	â€¢	mode="discovery"
	â€¢	intent="discovery"
	â€¢	transition_window + transition_reason (copiÃ©s depuis ctx.gates)
	â€¢	intent_eligible + intent_block_reason (capabilities gating)
	â€¢	docs_chunks (si activÃ©)
	â€¢	web (si activÃ©)

RÃ¨gles de rÃ©ponse (Discovery)
	â€¢	Ton : conversationnel, direct, actionnable.
	â€¢	Pas de â€œcours magistralâ€ : questions ciblÃ©es + prochaines Ã©tapes.
	â€¢	Respect strict des conventions UI (pas de markdown lourd / pas de code fences).
	â€¢	Si sources web affichÃ©es : uniquement un bloc ğŸ“Œ Sources avec 1â€“3 puces sans URL.

	---

## âœ… 2026-02-11 â€” Docs scopes + Discovery AHA flags (stabilisÃ©)

### Docs scopes (RAG light, contrÃ´lÃ©)
- Le **Context Loader** expose la liste des scopes disponibles :
  - `ctx.docs.scopes_all` + `ctx.docs.scopes_count`
- Lâ€™Orchestrator peut activer `scope_need=true` et sÃ©lectionner `scopes_selected` (1 Ã  3 max).
- Le PlanExecutor exÃ©cute alors le node :
  - `S: tool.docs_chunks` (capÃ© : 3 scopes, 8 chunks/scope, 20 chunks total)
- Le ResponseWriter reÃ§oit `docs_chunks` et les utilise en prioritÃ© si prÃ©sents.

âœ… Objectif : **docs utiles quand nÃ©cessaires**, sans surcharge ni dÃ©rive.

---

### Discovery : AHA moment (flags internes, zÃ©ro pollution DB)
Discovery peut produire des â€œflagsâ€ internes en fin de rÃ©ponse :
- `aha_moment=true`
- `onboarding_abort=true`

âš ï¸ RÃ¨gle non nÃ©gociable :
- **Ces flags ne doivent jamais Ãªtre persistÃ©s dans `conversation_messages.content`.**

âœ… ImplÃ©mentation (v1) :
- Le backend **nettoie le texte** avant insertion DB (strip des flags en fin de message).
- Les flags sont stockÃ©s uniquement en **metadata** (`provider.flags`), si besoin dâ€™observabilitÃ©.

---

### Logs utiles (diagnostic docs)
Dans les logs `heylisa.chat`, on doit voir :
- `chat.ctx.summary` â†’ `docs_scopes_count > 0`
- `chat.docs_chunks.db` â†’ `rows_count` et `chunks_count`
- `chat.response_writer.docs_chunks` â†’ preview du 1er chunk (safe)

Si `has_docs=false` cÃ´tÃ© `chat.response_writer.call`, le problÃ¨me est avant (ctx/scopes) ou dans la sÃ©lection des scopes.

---


